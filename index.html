<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>AI for those who didn&#39;t get it</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
</head>
<body>
<h1 id="ai-for-those-who-didnt-get-it.">“AI” for those who didn’t get it.</h1>
<p>I have never gotten along with the way that I have been taught AI, I am operating under the brave assumption that I am not the only one who feels this way. If you are someone who just wants to understand how and why stuff works, this could be the page for you.</p>
<p>The information in this page is correct to the best of my knowledge, if you notice an error in anything I have written here please do not hesitate to contact me.</p>
<h2 id="neural-networks">Neural Networks</h2>
<p>Sources:</p>
<p>NN just lots of line segments approx a function: <a href="https://www.youtube.com/@algorithmicsimplicity">https://www.youtube.com/@algorithmicsimplicity</a></p>
<p>Easy backprop: <a href="https://karpathy.github.io/neuralnets/">https://karpathy.github.io/neuralnets/</a></p>
<h3 id="a-system-that-can-learn-any-task">A system that can learn any task</h3>
<p>Pretty much any problem can be modelled as a curve fitting exercise, as a function from inputs to outputs</p>
<h3 id="finding-the-function-that-made-some-data">Finding the function that made some data</h3>
<p>You do this with regression</p>
<p>e.g. linear regression</p>
<p>Assume that the data fits on a straight line</p>
<p>To find the correct line, we need to tune the parameters of the function for a line y=mx+c</p>
<p>We need a cost/distance function to decide how good a line fits our data, for now we can simply take the y distance from every data point to its function prediction and sum them all up (the sum of the distances from the points to the line)</p>
<p>A simple way to now fine tune our function is to slightly shift each of our parameters in a direction and see if that improves the cost function, if it does then keep that change, we keep going until it can only get worse and we have found a local maxima</p>
<p>A better way would be to differentiate our cost function, and use that to find the input values for the minimum. This would immediately give us the closest line</p>
<h3 id="modelling-non-linear-functions">Modelling non-linear functions</h3>
<p>One issue with this though, most of the time our function is not going to be linear, i.e. it won’t follow a straight line</p>
<p>If you imagine any function that you like in 2d, a wiggly line, you can imagine how that line could be approximated by a series of straight lines that each start at the end of the previous line all with different angles, known as a piecewise linear function. If we can produce something to this effect and weight it then we could tune to theoretically any function</p>
<p>Here is how we would do that. To begin you must know the ReLU function, ReLU(x) = max(x,0), it simply clamps any negative values to the x axis. If you imagine this, you can see how this turns your 1 straight line y=x, into 2 straight lines connected at an angle</p>
<p><img src="data:image/png;base64,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" style="width:4.14851in;height:3.4217in" alt="A red line in a square AI-generated content may be incorrect." /></p>
<p>You can parameterise this function i.e. <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAALoAAAATBAMAAADVMwrkAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAMlSZEO8i3c1EdomrZrtpo7NTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACnUlEQVQ4EZWSTWgTQRiG32yS2W6atblZQewaQUXwh1q89OBSLSI9FEQKPYgrBStGTA+5FA8GTx6DXpQqrIeIgpLQWgQrGK810PSiqKTm4EkQai1eFOM3P+vuJpHUgZ2Z7/neeTLZBNjCiLndQ90iY+eHF54JDXv+izaZ64O48tsikhS088RWHNE4EWp7NABfAUUZ7V3nmGpU+OYxn/41BmRDq4UCigZYEehriNq3u7zO86lt2JKsqUZerrZcPCorPpNdnxFl2N5yLe/Ae7lZVvW8XFuoFxb2cVeUYfs2x88EdsozqdA9uYZpZA+i1qjDW0Wc/gKY9SGE7XEgkLp17P6QEEkP2zhgiZJSfITpiJ6Kp3oKBxeAUvoR/TRL2N5i1wGVGmkgYu2qzQY85gTOxg4R6BNQ2TnVPh5BIYssEhfdZJm/92uI/YCeCt/9ElTKSeeRwIAt/x7ylnRuUttP5rgt9D69jDfAS6zBjOTNBreXoG2k91ncblABlOnpp0ekwPK0fUIPotXqzWqVuvECvoE+H7rdQvtJjFkswwR6xd1Ljkli8WZ24DDluf0UPTIl7Deo5EPechzaurDH2yi9ZGzSyQuksPndPxXYJmDzuz/EqAVWozP8nEyR3aCAK0TSPo2k224XNJYDJmJnQI7XwHEg67rD0ByyRzZh5DDGPT22lyL7OzNnOpyqu+u4A2G/KmCQJlPA7frODzYSKSdzdBDaV8t8usoWvy+t0PWnXrj8SMLyUmTP7J3bzaFnN+Zdaa8LqOyCPmASAXfV1/Xq4MoqXsVmvB2t8s0IME3zZ9nyqVFISATWbJbVtsOiDgJz5xy/ffLv1lh8C1aRpU+zzZ8SdZmnuvR5O2pvIdQxYlgdcQiuhqr/Kpzu6Y6RP0noyEUDLqabAAAAAElFTkSuQmCC" alt="w_{2}\text{ReLU}\left( w_{1}x + b_{1} \right) + b_{2}" title="w_{2}\text{ReLU}\left( w_{1}x + b_{1} \right) + b_{2}" /></p>
<p>b1 decides how far along the x axis the transition between the 2 lines should be</p>
<p>b2 decides how far along the y axis the transition between the lines should be (therefore shifting both lines up or down)</p>
<p>w1 decides the angle of the non clamped line (the gradient), consider that if the gradient becomes negative, the line would go from top left to bottom right which would mean the right line would now be clamped and the left at an angle.</p>
<p>w2 further controls the angle of the line, however as this weight multiplies outside of ReLU, it does not get clamped and so negative w2 allows us to have our line at an angle go down into the negative y quadrants</p>
<p>So we have gone from 1 line to 2, but how would we get to 3? We can simply sum ReLUs that have their connection point at different x values, any flat segments added together will stay flat, but any segments at an angle will superpose on each other constructive or destructively depending on if they are pointing in the same direction or not</p>
<p>We could keep going like this, adding another ReLU for each new bend/line-segment that we want in our function, however, if we weight this sum that gives us a 3 line segment function (a linear combination of the 2, 2 line seg functions), we can change those weights to generate many different 3 line segment functions, if we ReLU 2 of these functions, we may get new bends at different points, for example lets imagine we these 2 3 line functions with different gradients</p>
<p><img src="data:image/png;base64,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" style="width:3.76667in;height:2.88879in" alt="A line drawn on a white surface AI-generated content may be incorrect." /></p>
<p><img src="data:image/png;base64,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" style="width:3.55in;height:2.63423in" alt="A drawing of a line AI-generated content may be incorrect." /></p>
<p>Despite the bends being in the same place in those 2 functions, if we applied ReLU to them both, the new bends would be at different points on the x axis as you can see e.g.</p>
<p><img src="data:image/png;base64,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" style="width:3.37507in;height:2.64167in" alt="A drawing of a line AI-generated content may be incorrect." /></p>
<p>The function for these would look as follows</p>
<p>&gt; <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIwAAAATCAMAAAB1LhWaAAAAM1BMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADxgEwMAAAAEHRSTlMAid2ZuxCrzWZUMiJ270QgZSBVWAAAAAlwSFlzAAAOxAAADsQBlSsOGwAAApRJREFUSA2tVYuS5CAIxKiJ+Dr//2uPh2TibmbnLrVWzcSgYNO0BODBiPWB088ur4huG9754OLVQYxsKK41lwFyGDs9aOzy/yt/ZR+FA0V/hgsS3h+ngSch2GtzMjOH2m3hF555aBBMFmzIDIcmPq3DqCtDT+9zecVsMW6fC9e3O5KlbFGzUlXH1Tef0NIEP8G9criNvhjTRxJ3S9mhetbGz9hOpvi1TggkkQlaiwVBisxbPo/PYJrB7VOJLJmIQcyYULHuxh+YZCZWQQ4ZWXIeKYcLuFoxX15hAXPnUgZiVU40LgyPyQepEbPWBVUz/r5KZmNCIsJGEmr0OzpnIiwFwms8iuEKZnXB5DibxKk0OVoroZKpXIzEJznOzSCQbZVMkaJ12sCbaGctgChGEeM+q0kr5GtFoPniUihV3im7tVQKSSUjZ24Bk95cg9DPVOch2RREtZXTuXqKUERnIvA7jePgf4PnXi4chb1lt16VTfiZXYZqFvVacU6TbSwQtB/lmWSUMgFs9C5X4QRT2Nv6BsdYmfniQhVwdCBtQqVeNaNdZiOsf9TMYaZqCYhXMJYemNAoEQZUuPsIM4KjBqrHOa5lAuDWcbpAJCoKZzZvt8TtytIRoHQWJSD9TcnslLY4xNe34hA6gZQXG90DJsbKRNweTi1s/crM4gLybSEwXete+A447+mCEr97SvQdQkx0mDvoa+VCkzaYd+fkSA5PgtWb2H1K5FMF2lS1R8ye/W2szCwuNQNVtbvZSYBOfzDK7E5XV2HvapjzFcx1A/cPax5i19Zy3fJP8+9u3bXE/eDbiBeWlsUyaJwypKW7FBePNy9FFf1m9aHZv4P9KV5X1Xza9j/riW/Os3FbkmehppdE/At6lRLqK1o+lQAAAABJRU5ErkJggg==" alt="ReLU(w_{1}a + w_{2}b)" title="ReLU(w_{1}a + w_{2}b)" /></p>
<p>Where a and b are the 2 segment functions we started with</p>
<p>But we add a bias, as below, you can think of it like another weight for the parameter “1”</p>
<p>&gt; <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKsAAAATBAMAAAAdX+AyAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAEInvmXYiu91EVM1mMqvVOvrIAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACi0lEQVQ4EZWUQWgTURCG/zTJutlu0rX1UOuha60gPSheCoogFk9BdKEgaIU8SlGhaOLBWw/Vg+DJRbyIIHtQkFAxd0HWgxQhxZxEEWwReiqmBkShqHXmvd3uJk1SfbBvZ/5/5nsvkxCg40o4Ha3uBvf1n5wY3S/LtEN/KJi+l8f1nx4pOanutGnrorVkDwtHgEnl9HwLcqxwcIG3ndfythLDJ2kS6CtJK8Lyx0BFiq2b1SosKiER07mTsPq81Jqx8sxYbRC+bZVWlWC6kTFGIWGHHSk1Y3eJqC4WbcMeU2Yc+5ikSey9CZjVU2jGJoHsE6S8AcF96Yvv+QUobORoG0891kPsQlmAWlEoT/nACD60YHVgSK8la73uUAmYwxo3h9jAWRiFOY6jrAfYnJ+z0Uc5DeE2Ej+g15pvOwu4RRSRFuUKzBK+cHOIVc5lJ2cTRY4hwE7BsJC0JLYAY6P8zGNsBgXqten5RM9hLMKEVkHSl82pev1lvc6udLIVs5R08Rs4W197VeeDf9EDPcAKk4hyCB/xgjSbskF67mJVYYsiyxW0gq9MOeixh2FIR91Wa3ANz5aG8M7l1OLbnseAB80nnb0Gga/xbZeRLrkkbGGVg0HrKnIO6wrLh9MQKD8HFB1nAoYgbLaBzC30c12vBYwnjsOXQ8A+22c1vK1y8BU6Hkk9mO0dmAJX6D/gdB7GnGdeWtIOfB9Zp9NmDsrj0x7woPr5jcXYTP75GTqGVjAE5aRrIjMmq8NfwnT5NVCVle03bSXUtfkwoneAVcpDKCZlwW2lfkO57fct8/4JEVXsjkJom5t2mCbUh+E0ulBoxt8z8eR/4lR0Qpu2jNdG/BdpqXuR6G53dKnvLzyOqBVKzd2UAAAAAElFTkSuQmCC" alt="\text{ReLU}\left( w_{1}a + w_{2}b + c \right)" title="\text{ReLU}\left( w_{1}a + w_{2}b + c \right)" /></p>
<p>So by making this step (adding a layer), new parts of our function go below the x axis and get bent, we can continue doing this with our resulting functions again and again to potentially double our number of bends each time (if 2 bends happen at the same x coordinate then that is just 1 bend)</p>
<p>Another way to think about these layers is, like I said, that they are a weighted average of the results at the previous layer (the inputs if we’re on the first layer), so our resulting function is a new function that combines the previous layers functions each weighted by their importance, for example if our previous layer had modelled a function that detected angry language, and another function that modelled sarcasm, our next layer could very positively weight the angry language and negatively weight the sarcasm to result in a function that checks for when someone is talking in a seriously angry way (in reality as the weights are tuned automatically the functions modelled at each neuron would not be human interpretable). In that way we have built complexity in the function in that layer, different functions can be made as different linear combinations of the first layer, and the next layer can add to that complexity yet again, from this we can see that all a neuron is, is a function that takes in the entire previous layers output values as an n-dimensional input point/vector, and returns some output.</p>
<p>This is all that there is to the function modelling side of a neural network using ReLU, we can stack layers of neurons to produce a piecewise function (in higher dimensions we aren’t splitting line segments but instead plane segments, however the process is analogous). But how do we make this piecewise function get closer to whatever data we’re actually trying to model?</p>
<h3 id="learning-non-linear-functions">Learning non-linear functions</h3>
<p>Well, we know the ReLU is parameterised by the weights. If we change any of these values that a specific neuron depends on, the function we’re approximating at that neuron will change, optimising these weights for our final layer neurons to have the best output result (the closest to the data we have/function we want) is how our model learns. To see how close our output is to the value we want, we need samples of inputs where we know the output (training data) so we can give a score to our current function that we can optimise</p>
<p>So we need to change our parameters to make our complex output function actually close to the function were trying to approximate. We call this process learning (as the model is learning to approximate the function). One way we could do this is to simply loop through every weight, change it slightly, and measure what that does to our final score. If its positive keep the change, if its negative make the opposite change, just like we did initial with linear regression. This would gradually make our score tend towards being better</p>
<p>However we can do a better job, it would be great if we could differentiate our cost function and find the minima like with linear regression, the issue is that our function will be too difficult to differentiate, we will only be able to calculate the local gradient at a point, not the global gradient over the function.</p>
<p>It would be good then instead, to utilise the local gradient, following it down to move us towards a local minimum.</p>
<p>If you imagine a landscape with lots of hills and valleys, this would represent a 2d cost function, if you placed a ball at some point in that landscape, it would roll down whatever hill it was on, following the gradient to reach a local minimum, where it isn’t locally possible to go any lower</p>
<p>Modifying random weights and seeing what changes is like moving around your current point and seeing what takes you lower on the cost function</p>
<p>Utilising the local gradient skips that searching as we already know which direction to go in one step that will reduce the most cost</p>
<p>A direction will just be a vector of our input weights, if we think of our weights as coordinates in n-dimensional space this will become clearer. Our gradient vector will show us which weights are contributing the most to the resulting error and by how much</p>
<p>This process is called gradient descent.</p>
<p>To minimise our cost function using gradient descent on our network we start by defining the cost function, this will be the sum of the squares of the differences between each output neuron and its expected training data output. We then take the average of all those individual cost values for each sample in the training data to get the single cost value of the network at its current settings.</p>
<p>The gradient gives us the direction of steepest ascent, so we need to negate it to get steepest descent (this gradient with multivariable input will give the change in each variable that maximises the change in result, therefore this will be a vector of deltas and so we get a direction vector to go in within our n dimensional space)</p>
<p>To perform gradient descent efficiently we actually do stochastic gradient descent, where instead of using all our samples to calculate the cost, we select some random subset each time (otherwise our cost function would be summing potentially millions or billions of terms for our large data sets which makes it very inefficient)</p>
<p>But how do we actually calculate this local gradient? Well first a few definitions. The process of passing a specific input point (set of values) through layers to get to the output is called feed forward or forward propagation, backpropagation is to go in the other direction, this backpropagation is how we will find our gradient</p>
<p>To explain backpropagation we will consider a single training input instead of a set of them, we will explain how it works with more training data later</p>
<p>In backpropagation were trying to find the gradient, more specifically the derivative of the cost function with respect to the weights <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD4AAAAcBAMAAADLixFKAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAELt2q0TviVTNMt2ZZiJHeKKcAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABjUlEQVQoFZ2Ry0rDQBSG/6RpGtNMDAqCeKG+gEREvIAQEDeusnEp9AmkK+mmko20FgvZuBRCERQVTN4gm24lLqRFqk8g2DfQmSYdMxsRD4fwnfPPQIYP+GPN2LCLx/xwjVMGiluwUOXbWU4ZqFiE7vCtYXEcQ30bAxTsQtMlHdDWEiGXg02cQg72Ed6QgLYZC/k9eliBqocYPR6ANipCPkRE8y6Ji75+AtqSI+RNM/Ke0G/F8p2j6I4CXXyg0dpDCx9eY0d6eaUN4gn32aDkN8KQBlqQO3Cb4wk+TwCQ+PO/fq+fK/8mZpZf7jCiG1gMauwzNsuAl+ICq2waW2ZmxSrTscpWzPLYLHGMZR9ba8RBZ4iFt9rDVWY5NZtEKiqKe5hExXgO5wgLNlLLqVnT7yL5bNmmPxV00ZNCOcgsp2aJ3Yd3AZSSaawj0mIVmeXUbMlraNY19PKSikspVI7mSWY5NWvWh2coDRLjneyO9BrZMNo5y4JMyWcvSysL8mbb9M95TSznzDb6PGWWvwHj3oZw9SjIwAAAAABJRU5ErkJggg==" alt="\frac{d(loss)}{d(weights)}" title="\frac{d(loss)}{d(weights)}" /></p>
<p>We start by figuring out the gradient of a single neuron, each neuron is just the function of all the outputs of the previous layer, weighted and ReLU’d. This is a mathematical operation and therefore we can write out this function and simply symbolically differentiate it i.e. <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAV4AAAATBAMAAAAqtHgvAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAZnYQqzJUIkTvzbvdmYlSEOKuAAAACXBIWXMAAA7EAAAOxAGVKw4bAAADmklEQVRIDc1VS2sTURT+ZiaTmTxqp6IoWGV8VTfK+KCiuIiSIriQAUWpXXQUqqhUB1GsomaUIoqoFdG6zEKquEr1DxSK0GVF0Z1kIUEXlUIRd9Zz751Xkgl0Fw/Jved+55zvfnPuzQRoNsVsxv4LRE5WkUqG248qs4katgJ313mJoTaDpxL3L0Mt43ViiIFKy0hdwKpbtVwsMU3UZ5No0gZyDrYlhTgmeS1D8cDq+KK1v8Q0QbDMSSDKuei08SAhIqA26tULkNbvWHH0TB8g95yAPjyy1soAgy4GYwd1fBZflPHgASK9Ec4LgwQxh41Th9Hh7HUZeu/rnU/1WQjTGvCIW6gS4TGM9BvpCVwAJnEYx9XJvJcHnpBevgHPUpwPeI43ao/Jl6FeH1eomBf2VgSpGEMhvXk7Y2e9U3NQnQFjJr1uNjEtDtKPROzJQU4uwvNwuiFNYROkCo7AVBck3OD9JcmB6dq8No8tOXRxJNTr4+nzEIVnykEFm0O9XgklyNfMVFWmUzMu0z4xC9NiGLk+Nwe5KhF+BnxDysQ7ZAy8pEtRAE4CJRuXLJHBxmUFehpjFAPkfy4eflX0dxQ4e0DIBUArI7COYvFtsVgVy+9YA0ktEwf1hejXBFmoSwtR7vjc3JcLfKJhGqQyYyt/UHLVeSDrAPuBTg/nghSa8x49krEN/RwL++vjXC8rjOmlxKhxMzgIiZ66Ckwwhl9sCCxKCxAxiz13/himvhG5sv0eu/ljoJMuQSp7g5ArnleyZGSAnME6EVqnlfc0ew7XORLpFTjTywtb6V3ADK5SHyxdW4AJhZoUWSu9gtv4Ta8wRi512jUqGiOpdD4dppHBvmrXVLe1ClkLeoX+L66ENyLj7nUkeiM39lfgpFfiha30jisvYAB96JamJRcpO1IbOwa2XfClBMGtVlK2IB+gemgVyAZq0OnzZejjntpIzYbsAKcveuinPYTpH1ce2IhRPOTLqL8CJ70aL2yl91HXseUWZNu9ffbxELBZC3jZHPa33wWCL+GCWzYzPvkG6i2kUFKMQiv4CycGkpttfD8E4Zu+o00FCJtDIT54n64CM92T+ewPjWnxGOgsRkcE0qcf8pCz6sNi9d4Hnfqg2lPlgNJQpP9cJRIfP3VjFbtiPrna4mKVI6XFv/FIQ1o8RH4Ou/2WVpQDdF8awmJ5S0zpxGBbQfaKaDbd4dj/p1eym8UyJH6syRntQemH+g9GadEOeCWFFgAAAABJRU5ErkJggg==" alt="max(0,\ \ w_{0} + \ w_{1}x_{1}\ \  + \ w_{2}x_{2}\ \  + \ \ldots\ w_{n}x_{n})" title="max(0,\ \ w_{0} + \ w_{1}x_{1}\ \  + \ w_{2}x_{2}\ \  + \ \ldots\ w_{n}x_{n})" /> can be differentiated piecewise about 0. This gives us a function by which we can get the gradient at any particular neuron (by plugging the values at that neuron into the derivative)</p>
<p>We want to find the gradient of the total networks cost, meaning we want to decrease the cost by some amount at the final layer, therefore the previous layer needs to decrease/increase by some amount to make that happen and so on, so the change at our final layer that we want determines the change we need to see in the previous layer and so on. If we do this throughout our entire network, we find the change we need in every weight to get the change in the output we want. Giving us a vector of weight change values that is our negative gradient</p>
<p>The process of taking our wanted increase of +1 and going back through the network calculating how they need to change to get that is what backpropagation is and it simply uses the chain rule to do its calculation (think of the outputs being chained to their inputs so shifting the final output by 1 will pull the weights by some amount which is what we&#39;re finding), backpropagation computes the gradients of the cost function with respect to each weight, and gradient descent uses these gradients to update these weights</p>
<p>The chain rule is just the statement</p>
<p>Dy/Dx = Dy/Du * Du/Dx</p>
<p>We can use this to calculate a gradient for the entire network, simply from the gradients of each individual neuron in our network</p>
<p><img src="data:image/png;base64,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" style="width:2.40964in;height:1.68333in" alt="A diagram of a network AI-generated content may be incorrect." /></p>
<p>If each of these squares is a neuron, we can consider each of them individually, and combine their results using the chain rule</p>
<p>We know how to calculate the gradient of a neuron with respect to its inputs already, So, considering layer a and layer b, where a feeds into b, if we calculate the gradient of A with respect to its inputs, and we calculate the gradient of B with respect to the output of A, using the chain rule we can compute the gradient of B with respect to the inputs of layer A</p>
<p><br /><img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAIMAAAAmBAMAAAAVYjUIAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMARM0iiWZUqxAydrvvmd2xbVBNAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACjklEQVRIDeWVMWwTMRSG/5AcTY6mRRFLpzAy0Q4IJtQgpRNF6QBFAgFdyJrAAhPKwJ6wgQpKkKqyILUDICEB6sCKYIClSDQbA0OvREBKVZVnv7M5v7tCEGLCQ/z8f+/9Z/t8MQCkxnHz9dtTTyiUrRBkp7fL7+ekjkJA0lDD6pPANDDZsYIN0gEyK0h/t4IJCADNjhmCLB4CeaWKRpn+CrAmZEBbnO9anS0yfSvYILSoNKwSBtpisWdlthhZtoINQotPVjCBskjPvQyHB87U1EJyzw22fWpqVi9k/o2VONAAPj7w0KvjErBUvTwl8oCz8Mjia/X0jEAakEW7pUGzFW7n0EGRmN1Uu6a286pLGOAOhnl6xUZokfviJiLVNxZtdxoMsFA+SfbUinQi9EvFth7//EkFxmKUn2YQA3SQ29BSc2K3WWR7xkLMgkGKNuKjtvDrWGqo07mna55i+gfw++p04rBRwl6DfTSosHCuWnuqvpFHIg/wFqtbt9U3UhJIgVvjh+CtPxbkfxkep4Xe/7vF7q3DLw1ikb+mWmLqBSwM4vCrHG/VPaVh7k60BY5BlDC4wV0U7Lg1jkHCwFstJahx6Z/uBb2RzGDTiE8sVI5RfyKR0k10ZP3dboj063Ema+gfJ/xoY7mEkI1fRPpfKlrzGwvvRcz5Ty382kTcQzxWDKP5aiFHh0tRiWOnRt9EFcz3Zq/ITI3QyXcTga3hm6iiNu2uyGSETm4zEdgavolodzfQbLipjHItvHJ1iBq+iciih1Fhwcgvl58JC1FTNC8owUIjH2jvdz1EDd9EibNgNAaM1F0LUcM3kbYg4jRGy0AmcHTIGnUT0Tdy79vYmnyrChU+N3Bxa8b1cGp+AO72/4v8FJ2IAAAAAElFTkSuQmCC" alt="\frac{\text{dB}}{\text{dIn}} = \frac{\text{dB}}{\text{dA}} \times \frac{\text{dA}}{\text{dIn}}" title="\frac{\text{dB}}{\text{dIn}} = \frac{\text{dB}}{\text{dA}} \times \frac{\text{dA}}{\text{dIn}}" /><br /></p>
<p>We can continue this process to have our gradient span as many layers as we want, from the very input to the very output of the entire network. If we just follow the negation of this gradient (subtract its components from our weights), then we will be performing gradient descent.</p>
<p>This is all good, but why is it called backpropagation? Well imagine that you could change the output, and it would change the inputs to the ones that would form that output. If it were that way around then the inputs would be a function of the outputs. That function is <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMAAAAATBAMAAAA0WB+1AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMARN0iMlSZEO+JZnaru80cuAg8AAAACXBIWXMAAA7EAAAOxAGVKw4bAAACiklEQVQ4EaVSTWjTYBh+0jQmTdokMJR5UCsoXjMPoiA2HpR50EacqzqRXqZsXoIwph40g4mwUwXnYRT5FKZYlFXGRCZKT162Q2+CIvQiMgUpVBh68s2E5fsysg19D8nzvM/7Pd/P+wL/HWo90cJIVDYtnOwgm1yslkmT8vQRQvUFui6RArzkC0Y5Qj4XiMo2l1uBueQ7x0thNNDik584Qj4mRyNouRHeCOWYwvianxwhn4zH8VXYu4o2BpaTc7iqdMAR8pEaSPdDmb/53MXAbK0ckvL2H1NcVRzeqI1K9w49At70OTDmzkGjCmX+Po6V0S89XGKiT4ARze3pZUpTLf3GMJFsHR9F0y2LYVT+JuWv6oSyv1JFjmVbVLmIFAlXcVD1xrFA2ZhPG3YB3mPqlUTzULWL0Fx0xA0EZnrolLr9LrxAtqI3yfMIoLdgHU23MQ7LifncBR4AQ6Bz0zy8RxUF0E5h7PhMEbsMsItkn5ZgApqtMfI8BdD/tptpyG10I+azD1SCMdoZOQ93sJtOlmmE/gnxLpS/0HB3UETBoWNfA/0xg5RNF6oi5hNAydu08yUJli/lsYwFxWQjgrvQg8swmboMkPNbqQijaVOTi8B3Wp+te99UT/QJkKkwesIhHUXfdNFWx3pSbl3YQCAXMeMoLUoF6pKu4VaFmT5Srt6Ahetd9h5D9JGbSE/6BsOJEl5NzwJPTp9nyhw/2YI9kJl+DSM8wLazx59Kg33DvuFBnqxRpwe3TmHgjOijs2j9mn5GkoDomYWQGwIVhzznR+J6wxlVAXt5EuIPYkLwORxp1LVNhNFaaYBQeUBgos+VSNv5K8LJSBl8tkaUPD4l+OguL/0zdhJXysAfCIq+UfveAsAAAAAASUVORK5CYII=" alt="input\  - = \ grad(output)" title="input\  - = \ grad(output)" />. The same way that we can model our neural networks function as a graph of our neurons, we can also decompose this reverse function that way too</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWkAAAD9CAMAAABJJXadAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAL9UExURf///8vLy1RUVGZmZvr6+qurqwAAAAsLC/b29rGxsRsbG/f39xAQEHV1dXd3d1dXVyUlJczMzCIiIvz8/Ly8vB0dHQoKCl5eXs/Pz/Hx8ba2thQUFJCQkOTk5KSkpCYmJnx8fOXl5cLCwhYWFj8/P6ampr6+vg8PDz09PVxcXNnZ2fDw8ISEhBMTEwEBAXJyctLS0vT09EZGRiMjI4uLiwwMDJ2dncXFxUBAQC8vL5iYmOjo6PLy8kJCQhISEh8fH0NDQ+fn56+vr46OjiAgICsrK4eHh/v7+0pKSrq6uv7+/tHR0erq6unp6Y2NjSEhIbu7u/X19UVFRVhYWLKyso+Pj5qamqCgoHBwcFNTU2pqaklJSVpaWhcXF93d3V9fX0hISBEREX19fdbW1uLi4llZWfn5+dvb2zQ0NBoaGk5OTp6enggICFZWVv39/a6urgYGBmtray0tLYyMjN/f34qKijU1NUxMTB4eHoaGhuHh4SoqKjMzM4GBgWVlZWFhYcHBwdjY2E1NTQkJCYWFhZSUlGNjYwICAg4ODri4uL+/vzIyMm9vb6KiohkZGc7OzjExMezs7KOjoxwcHNPT06qqqkdHR9ra2g0NDWxsbMfHxwcHB4iIiCQkJG1tbS4uLpKSkq2trdXV1dTU1JycnJeXl5GRkYmJibW1tXNzcwQEBFJSUn9/f9zc3LCwsGBgYDo6OhgYGLOzswMDA6enpzAwMD4+PikpKfPz80FBQe/v75OTkzg4OMnJyTw8PODg4ERERHt7e11dXd7e3u7u7np6esTExHR0dHFxcbe3tzs7O+3t7evr6xUVFebm5gUFBVVVVVtbW6ysrMjIyDY2Nm5ubs3NzZmZmcDAwDk5OXl5eX5+fiwsLJ+fnycnJ8PDw+Pj41BQUE9PT8bGxmRkZPj4+IKCgigoKKioqFFRUaWlpampqbS0tGhoaGdnZ5aWlnh4eGJiYsrKypubm729vdDQ0NfX1zc3N4ODg7m5uWlpaYCAgEtLS3Z2dqGhoY1/iFMAAAAJcEhZcwAAFxEAABcRAcom8z8AABUcSURBVHhe7Z15fBRF9sBL1BcgkQXCfQhyiIhkYbnvQxEIROQSUSKn3IYYjOABLKAERG7FKIdcuggsAkEElPtaEImLiooIoqisi66Kyg/9rZ99r+YlmckcXT3T052Z1PcPUlUzPZn+Uumuqq56JTQajUaj0Wg0Go1Go9FoNBqNRqPRaDQajUaj0Wg0Go1GY8w1RVS59jo+RBMU14MqMUX5EE1QFGOPxhSP5UM0QYGm424oYciftOlQQdMlORmIUtp0qKDp0vGcDkAZbTpUtGm70KbtQpu2C23aLrRpu9Cm7UKbtgtt2i60abtQN12Wk5rgUDddrnyFipzTBEElZdMAMZWrVOW8xjQ3mjCNVKt+k75cB0WNmuZM40WkVm0u05jg5jrozpxp5Ja6t3KxRo16t9UncSZMl73tT/Jnwp8b8AsaBRr+RVpTNd2ocROo0zS+WXN5UOUWLVvxaxoDWrdBYW2LKJtuJ9rjAR2E6Hj7HVJ2pzs786uaQLTugrLqVu2qbLp4Yjc03KU75pLuaiFd92h/t+tljX969kJTva8TfdRNx4q+eMw9Ml+v371NpOz+92Et1/in4v1oaQAmTI17FB0AkNyMix4YOEi6HjxkKJdovBlGkh4cjilzI0zD8XZYfwSX4cEjpetRo8fU4xKNJ33Hop+HUihpcixvXCpACVcR0bn2w+2k7DTXRUXjye3kRtZo06bFeDx0jCxiHkkvTR8Hj06owCWaHKqgl4mPcQvNrOnHnwB40vMmWG/SZOkapvyVSzQSEp2cW//MmhZT8fBpMuVGt9HTSXWTtKce4BKNeBqNTOzGmSBMJ87IgJmzEmXajWeqzCbXUPzZOQofVxigGj13HmcQ06aFmA8wdgGn3ai4sMsiKXt+H6//h8JHA2wQw6LnOEcEYfr5DIAbOO3J1Elx0vXi8i9wSWHl1h6oYVQpzkmCMC2uzwS41/dBSS/2l64zH36JSwolz9Ff96SpnHMRjGlBg4B+b31dBy2RrrssTeKSQsdQavg25kwOaHoZJwPRzMP0I2kAI/0/Uuy6/GVyDYtWrOSSwsWqsnjyd3ImFzS9qNtqQ9Z4mBav4Ee9ymlfJK3pVFPKrlWj8N0dVzfCE/8bZ/JA04q4mxbLsfG8ltO+GfraOnnY+g1/55LCQfzkynjWSznnxkapQ4Vkd9NTY7CFYVBdp76+SR64+d7VXFIYGEin7HXpQLYsUaW0u2kxKwvgRU77Zys9psG74/IqrlGWqCd+Cp7uG+M550HRYarc5FmFtwEMHsZp/8RWmvCmlL2s9nYuimpI9I6dnLGI7qi6pGeL0Tcd3qILF8DbD+7ikqglhS4duy0WLcQe/NS9nA7Mzn37E8j12N5zDmC2zBxVIm0cdgae5I6DnLGOzodSIbOfQq+HODzA9cSgx5GqYodMqfAPPjoyOPogfuVj73DOUkYCZB3ntCHXrakl7cXd1Un+VGEfHxsRPE+t2ubhmY5b5l2A8uof3XNj2kSXQdicZszMyDLdcQnAzEnKFc8kk1CaqfmQJzbQRECfDXsv+keU6euzsS93lDPWs2AQtj/yniqocOC1bYN9t+zzE1GmO45F0WU4Ew4qYP18j9PKdAF4jJOBWB9BpsdhPy6zEmfCwnWNAaY/zRlVFkeb6RP/nI5N2FmcCxfVsVaf5LQi0Wb6xKPoIDPweJsFjBgFUMtcRzvKTCc9iaJ7hF20EO/j79nPaTWiy/SJD1DAh3bMb04cDdD2FGeUiCrT75DoezkTZmKxz1fZzAzIaDLdga7RxqPHFvFRBkA6p1WIItMff4Kin+WMDSzHX2eiexQ9pqnhBfdxxg7ooXub1pwxJmpMH0LPjU5zxh4a9ABYTAtglIgW05+i6DOPc8Yu0vGXfsZpQ6LE9D4857O2P/ePpQUw5zhjRHSMe2xA0W9+zhkb6Ybtj/OcDky9hfWjwPT7D5PovBU/9pE4JwsSPuVMAD5Ol9OsI930ORpmP7uHczbzMkB9o/7LqS9iyHPEmz78Lp7DGafmZo3Apt7bAR91NaNHxwAZCZFu+ss4gKw1YXk0qwTdi8dx2gfNishFudnTXtgR4aYP0+TOppxxgvgLWRB31Pdcve0V5HQ1OHPP3RHf9ngf7+gZHgsF7ScNIMbX9WNYlWRZn4+tkR3JiG5Pd76ZZmSt4ZxTnMT73ZGvOJNL9+olSTNU28ovRbLphhPWASQod9LCxhEU2ofTTFIV16y8T57O3Tojgk0PH42n0qMArNx+/AOAkY9whqj69TekeW61i24DUJFruidFkPhXQ845ypf4TaZwWohvH5tLnmNKDPO4T0as6djzeDbvFYyZ4EWPrIPBF2UyvszbpBlGLc8f/iNSTcdOwNP5wmO+vpMUx2sF/kj8N30tgCX7vMcGItT0OarR6wtO6KlLmZD13U6uz5u/p/nS+cGLXQSa/ohO6HxPzhUEapQDoOfFkDVojfuy6TzaR6Lp/0jRirPF7aEUDyJNu9v33nMpP6Q6Yzr2JlVaeXd0qUYnXwypRu86r8qPKrv2PT6lrfTcxe+cns9pIYYTpjeWU6WkV+/rHvzOc9/nTJCclGJUUNi1r9sPMqAK9qL6cYkXrY7R606YVt+fcHN+0zTPovJPnAmWINfY+uD41MlyBXOjOfcBjPUXKOiy/Cwn5k+vlL9ZhbL5TNN3nhvywjM0HXd6nyEDjEyn9H1CfsvSt8WKU8l+m0M/uwKAOLEmAE1P/MUYvPV5mh6xCa93bUNf4YemLdi173g/1zKhJ/reRNktmKzr8wEMtrKbVMYa0sKYOtabPsvJQPw1n+lfaXws+RrOhQCaNh/fIx+db58vPY8az6MbFfdiztes6tcAMj5+SL5ZBatNlzRY6U7M8jR9gETvzYl7GQqhm674LUVZgJjFT+UFdytKC2C8Q1SfwOr8triF3q6EpesRgzI9lc5tEmdCI0TTiYcHXiEnMd8d5hIXP2HZPzmdywN4KX/7hOjaTxW3yFyhE4zpA2fxPP6PMyESmumr/SluJ2T3/pULcrhufBYseYszOVAcZufi3gdhugJF+dnAmVAJxfQ1NC4OMPN1z/rs4g68j3i0P1K21oHsPzu3abl50xey8ewsqtGhmL46wBWU5sIvXODJruKQMM19AcxOrP4tOO0Epk3/RmdnVY0O2vQjFe6n/3BIK+a3ofkivuwWcn37tdgqDeuSPQPMmsbvC5+0lGWWEJTppEmu0Y3dgZ6otb4fv3NebD48UajOaUcwZzoRO7qwycqw2kGYHvG7a0eGD8YFXh13FftWyTnDX9Tu2KY8xTocmDP9PYm2NOCfadM7S2yWnkcaz+JpmQWQLuOJi6rYAZgdrkABapgw7bpGX/ERvDUEzJlOOZouo8ROHPStSrQJ+r6uZesUuPYjmXIMddNnT9KjuWMWh7BUN11WxNamDTMA4m5TnDg8tBr2ZKlmbMHb5xFXmWOom86mIfRtVkfoOqpsuu1C2RuEuc+qXwXuxvd/KETsXoCa+Ts3dqNumih5qhXHVLOKpsqmXSR/asZY67oZMLeMoOj2jke2N2c6tQ6HCbQM/LM2Ybr4IZPtns71sc3drSbAYoVfEl7MmQ4LJky37W92IvyYLIhB0ROcn2Jl0rQM7WwxiqY3y9tho3STIxcL5Vd2n6znEOqmM+kp3f4RqzhOsUW8pWx6Wc8pcjvFby6bav7ED8ZjDrla1Y6ibvqbBtj6T7WwIy65qmx6VLyY95qsoGlLebBLhYO0zKm58XP1sKNuepH4lXoN3iGkQ8JkH/HAfhmGbeIY1fU0C77IoAMCLICxC3XT2BufSgMOt3ORNZjuje96MZPUzfYZ+deb2vhe7AjMDHW2ROiYMi26URDF6qu40ApMmxZixED5oOWISnzXUvhn+Pr1Me6zqp3CnGkxlFS773wTKkGYxnpNY7dQeYrhItPPaYlkKbEQ/72NixzDpGnxC8W8nWjdzshBmRbi62rkOvu0wZ/XJICsyyni4BsA1RxYze6BWdNiT2M8xc3mQlwGIEjToupb0vXmawN1zv+N7/idEt2vALzh0DLrHEybFmIFfv92vh/emSdY00L0vFNG3k29we/A3vFPciNg1MB3fi9TjhGEabmTZHGL9pMI3rQQN6XLuXjJ/kI+9cYX+ZZS9IYMqONaAOMUwZgWd+EpWFSrQzGN3ZIq8gFM/7U++oCJP9eHjLo5K53isYc7kdPOEJRp17adlmyJHJppIZJoYT5kt/d+6t0Qu+F5Uw4TL1WGrMlOLhMJzrTcTrasFbfFUE0LMe4WekQBo3/O9zEU4mwhp4nymA9xWn1IBGlaXqvvsOAxRuimxfCVNA0Msv+/GBdITgNM94i3fmITQJoT8XSYYE3LcF7FfS+OMoMFppGjcrUWLG6W+1EdUgE2cZoZh+/4L6cdIGjT8rZYOuSOOZo2v2ufN4lbZb1OeIjvgK2wgfdNvo0/U37HNqHfBTBhB01/w8lAvORtWvwNTy3k2yKaXjSPFx4E4B4D00J0nkXBVOHly3L6zBhMesc6rYtdxl1F+SONsfYZL5pue4gXkwQAm6ZepgX1FhuF2K5G04oYmEb6yvd1Gp8inqkDMMP7ovQTtgl/6yffpYLKH5s6aFoRb9PyWn2Hyt5X/uno+nAFahqaFlMfjKN3Vjv5EEA5X+19ulTLBzdKqCzAUWcrf6oxmd6mxZ1Y3iOk1SCXeLWjMcuMTQux60M5oortDp+zDnYuli3CDP7IQOAdVWUBkDov8QpWYy74elRKtRoGhrBAP6Q1vr54wVWvy530OZ1su3xk04k/MhB4Z7DWdKhcor0eX+dMwWChfJ4FvXwOk6bTk8jZnAkE3kAKlmlRjzYO218Anj3nsl+KBmhy2ldznyYXvsHpQFxf4EyLem3wu19Q+9O2gcTacZA95DO5SZ+v+B67JkasaXFwN57UQM44TsMYgDeF6Omazu4jZs1nkWtatJa1umBcQFImJ0CMHNgbNp4W83nHYZoTwabFAlq2+iNnnOUCwNifOV2RY4tt4bwL7LlErmnRmlRPLgC1+u/YnO7FacQVLy+7zc2cJyLbtDhOLZAZnHGODiUBWnisG0raJvsqRcrkVoMINy120n71PzrdAqGHyfk28YvdKOeopt6fs+NLpJsWC7bh+VzLGYe4tM7nHlQrXaPX5w/Leh3xpsUC2pnoAmccYfhegGRfY4vDLw4i1dnr6d4Y+abFQbot/sYZB6hI+20/w5l8pFRyBVtpuYCmjkW6adGdIi/Wvco526HAUH/xG1818Q8aoYE3LzeNAtOiHvUWU1dyzmZOlQaYFmiaQaxrlK94NJgW3Ska3XTPnoJd0INEg0ffu17MItfRYFpspwjy2R05Zyd9y8GSKoarjW59j4JxRoNpIdZiLy3GY+aFLVQsB5DG6YCc+zFaTIs+TQDW2a36ndEJ0E4tet/JqDEtl8OXs1k1hUcdwmkDoqA9nUsfvBaOtTUQDy1n+Zdi8IBoMi2KLcFanW+7j3DSoT7AkyqPzomoMi0q4f0pxsQmviEyCe8Mf3DakOgyLcZhC8S2djVKgUOcNibKTIs+eAFpYk+t7r4JpeSb7RiAaDMtVk7HjrnfGPJWQrMOTCyhjTrTouM6gEwbVP/8LkBd1dshEn2mZa0e29LaUGTe3DzT5FLlKDQtxtF4zrGdnAsTvQAyHuC0Eiuj0LToSBM/t4U1IODpLFhy0cxD+WfKR6NpUYGCRm5z2zHPao5jd/QDTisw7Lnz9J8fhaaFoP2Qd4etVh+/gj6U12Md/DoZv020mk68gGfWXCU0ZjBQOImnOG3EnqVyPwGI4BmQgUmkXZGPhWVz7MS1MyHhO6WLdMr26nI9f3baH2Oi1bTrArLjIGespEM2wBVOB6bSo2XJ86j79uykGZAjuTgQsyLQdArV6t3WN/a++hC7Rhs5E4CvVtL2rwB1NshZp9iebrvfmG0RaFqIKXie1tfqG/FTv+Z0AE4ekzPzKn/H272gaUUi0HS8VG1xmN8HWgBsGsYZfxzYL2flZbffkhvQqCUVKFGTj4go6ALSxtpa/R5+pEE8qAquSIZQpJLb1Mz3l6ti2QYfdiJrdRsrw2tsjYPsEj6WSOYx9XW5qXvGthpcUDhIpLHN5tbdFothuyPQGtjO41+T270s2+DE5BNHkS2QXj465knNVHEL0xffHNsd/h80JK2Re+fDmRV5u54VHuIH4Kkfe4VzedAerWqU5iOQyXiXy1nO4sXw9Jfl+xetKBAb+9tPilyS6RWBtqm0osKbfAQ2KTLxqs/p/HT9w7XdS9pSizePiCA639gDDdzFuRywd7yurTFNAJ7gI8TxRgCdfA5bpdToLzVnPnxjuB9BFGwa0NLXxpxh0PT8g8Zg1c81/TR+ShVOe3Dpi1TynFpe/QFutPIrjfN4SkLTj3IyECfzTB/FVsXDnHZnl4wQBHE/fskFhZpuXiGs0bTKiM/FXNPDB2H3zStOyp4qveXk6F7PWhfeNrJZRYHZ3cN0mzVdrzfA5vxb6I44Ipfkw/y1fpdfFD66kRO3C4hZ0/j+/A2YDkNkgBSoNYYLNJJfKBZpXgvEpOl5tJzF/cFCz3ND5CB/o90bnQymWSB5jmr1q5wxa/oLPNYtSt/wy7SOGm+DS6tyicaNCqPQTU6tNmf69umQuiJ3OUvnlWek5x43nOASjSer6bbIm1SbMt0TL8iuyzWyYOEO0jy2/tIDXKLx4hSpdl1AzJjePi0BFl3jKqgxw3UbfPAlK3d7jT4a0AVE1mp1050oSnqCa0RvaJHppLncaD/LlzW5yFr9w2ozpq+UigNYT5mX7nMt4az7sXxNE5AKNKFo0Soay1M0XactQIt48Xn6FXndOHOjk4G7I4l51AwufaCYsmkkc2Xi17L1DGdeLTCB4wo+JyjYb4sjZkxPfn6Z/Dl7TeEeFDXNUqnNhGnJ9JKv6NENs1CtVjNNq2gllS98Ho6pZ1EPxVVWMZ24RmqGuP9atTtSoYNi4CuYbi23yGky2eHNsyKaH5RMb68G0K6rhdsFFkK2ql2nayvG79D4RbGP+Lzb0JImKEyN5WlCQJu2C23aLrRpu9Cm7UKbtgtt2i60abvQpu1Cm7YLbdoutGm7QNMqy+y3aNOhgqYHrzfmA206VNC0Itp0aPyHPRqzmY/QBEep06r05SM0Go1Go9FoNBqNRqPRaDQajUaj0Wg0Go1Go9HYhhD/A1aE9riF4cmLAAAAAElFTkSuQmCC" style="width:2.40694in;height:1.68681in" /></p>
<p>But what happens at each neuron? It simply performs the chain rule for itself, multiplying whatever gradient it receives as input by its own gradient</p>
<p>But what do we do when we receive multiple gradients as inputs? Simply use the chain rule on all of them and add up the resulting gradients</p>
<p>We can do this until the final layer (the input layer of the neural network), here we just subtract the gradient we get and we have performed gradient descent using backpropagation.</p>
<p>One last point, when updating our weights, the size of each update is scaled by a hyperparameter called the learning rate, controlling this is important as, too low a learning rate would mean we would have to perform far too many iterations slowing gradient descent dramatically, but too high a rate and we might jump over our local minimum</p>
<h2 id="recurrent-neural-network-memory">Recurrent Neural Network Memory</h2>
<p>We want a neural network that can receive a stream of inputs and, instead of taking them each individually, evaluates any 1 input using the additional context of all previous</p>
<p>We could do this by, for any 1 value in the stream, also taking all previous values in the stream and using ALL of that as the current network input, this would work but would he inefficient as we would have to continuously modify the dimensions of the input, therefore we need a way to collapse all previous values into a constant n number of &quot;memory inputs&quot; for any value in the stream</p>
<p>Well we have this for free, when we use the neural network on the first value, we get neural network output, this is dependent on just the first value</p>
<p>We can just pipe this fixed dimensional output back into the network as input along with the next value (when we have no previous state we simply pipe in all 0s), so now the output will be dependent om the second value, but also the first as the first networks output was dependent on the first</p>
<p>Recursively, for any nth value, using the n-1th networks output as input (the same network but its previous iteration), we add dependency on all previous input values, and therefore have a memory of the context</p>
<h2 id="lstm-neural-networks">LSTM Neural Networks</h2>
<p>RNN has the issue that, over time as more context is remembered and piled up, the network becomes less effective at learning new things, this is the long-term dependency problem, this happens as in RNNs, the gradients passed backwards in backpropagation can become very small, (the vanishing gradient problem) meaning our model struggles to remember information from further back in our timeline. LSTMs structure is designed to maintain gradients better</p>
<p>We fix this in Long Short-Term Memory by adding internal state to the network, this state is fed into the input of the network, just like previous output is</p>
<p>This state is modified by 3 gates at each step</p>
<p>The forget gate</p>
<p>The input gate</p>
<p>The output gate</p>
<p>The forget gate tells us what information that is stored can be forgotten as it is irrelevant, works by multiplying the state by a value between 0 and 1, ranging from forget everything to remember it all</p>
<p>The input gate tells us what new information we should add into our stored state by combining our current state and the current input</p>
<p>The output gate tells us which parts of our current state should we output in this instance to be fed into the next step</p>
<p>These gates allow our model to selectively retain or forget information, helping our model only remember important information which reduces the long-term dependency problem</p>
<p>The gates configuration is learnt by backpropagation of weights during training the same as the rest of our model, it simply learns to minimise error, having this additional infrastructure just allows our network the system to learn to better remember long term</p>
<h2 id="cnn-convolutional-neural-network">CNN (Convolutional Neural Network)</h2>
<p>We want to use images as our NN input to classify them, however with 3 values per pixel and an entire screen of pixels, we have too many input dimensions, we need to fill our sample space quite densely with training data in order to accurately interpolate for our resulting function, imagine filling 1 dimension up to 10 units with 10 values, now imagine 2 dimensions, filling a 10 by 10 square with only 10 values, now imagine a 10 by 10 by 10 cube being filled with only 10 values. Each time we add a dimension we need exponentially more samples to get enough space coverage for accurate interpolation</p>
<p>For example, say we had a 1920 x 1080 pixel image, each of those pixels taking 3 numbers</p>
<p>1920 x 1080 x 3 = 6220800 dimensions</p>
<p>This means we need <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADYAAAAQBAMAAACvnpHFAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABCElEQVQYGXWLsUvDQBTGv9yZyyFUg3atXHFwqRANgiCFQjeLkKlzaKcKQsZWcCyIUwcHx4ggjv0TsnYoHC516HBD/4AuzvHhpSDBPn7w3vd+fMDvsLPkcAmid4B+E6KuCKvwxU+Uu1CuTnmgvWAXIWHdzhTC8HfDPyKRrqQ/xi1hnfNaAyoalc9MTGanaGBAWMfXTKPTQucm8yb331hiRBQ9I6foAl2WiefoOvvbE0YaFicsHkbyzYcZ44mwPRimH6EIwy8V9B5CAs5x+yLGQ9PL80We614N4XlL1DWB4SBw0qJdXv4VWFR+bvIcrtrc5f2C/aT8K7K3xpHc4rjBnG9xLMBd9X/3A94vRNaaVQ1gAAAAAElFTkSuQmCC" alt="n^{6220800}" title="n^{6220800}" /> samples to cover the space where n is the number of samples needed to cover 1 dimension</p>
<p>If we were generous and said we only needed 2 samples per dimension, that would still leave us requiring <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGsAAAAQCAMAAAAmhHE8AAAAM1BMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADxgEwMAAAAEHRSTlMAdomZu1QiEGbdMu/NRKswCRkRigAAAAlwSFlzAAAOxAAADsQBlSsOGwAAAe9JREFUOBGlVAFyxCAIRBORGLX+/7VdQHOXaXtt55iJqIgLC4boTclpxw1FJBLtrJcliZvqIMnWfRl08y1JqcIfQCx5qHwEoo51bCSYUqjkhvwWjjmzYtVMLZUGxFawlp3y4VczYnHD+1BkWNsZkApEbIyR0p6SbknWWJYhxG1PtsbANUpw3tfWXbdabKMFAUsQw+J9oCwgzojKRyYZoA+Q2bHcEFHAPDweeB7j7FZSdf0iHMowLGWKqh5UrNwp72gIaJWOI1rGdlAiw5qGQ323yS7xlaF5fTM4ljERtN8US9RLtREZCjFpMK0GkXAiDzeQTikiY5M7Flvzggxjy084lo3txBYjzKYNB5yuuuEwNLCshsaAGdyfdlhMGJF4HWyZjJRZdz9hKGUofWWg/eIpTIL4UaoN+bE2PnA5WjK8aTJqmMIeLFZZIzNa3STI7AZFntHE+rmy8+avqq7WcNOmv4IpsllV1pIcy/Nqhvgw/WUW71AUtA5L+mqbuWF5OREJHP5T5AnqUGLD6hTMkZc113Wn0z20W9JzTNeBV5Om1y84ezJPHP5QL9rVIfhzenX53Va21lpCC3AHI4Ivn1fJvQ/xy3mIZURNS2qBPSy/z07t0QGsYhCCX9YFxTPw6xXk2EeN+vpTSNfu7yB24hNvYw1be/7y9AAAAABJRU5ErkJggg==" alt="2.5 \times 10^{1872647}" title="2.5 \times 10^{1872647}" /> samples, even just to store that many samples, if every sample took an atom to store, storing all the samples would take more space than the universe provides, in fact, if every atom in the universe each contained another universe full of just as many atoms, you still wouldn’t have enough atoms to store the samples, even if you nested these universes 23400 times, you still wouldn’t have enough atoms. This is evidently a problem</p>
<p>Our solution is to convert our high dimensional data into lower dimensional data, we can divide and conquer logarithmically dividing up the image, with each exponential layer summarising some information about the image, the result will be smaller dimensioned inputs that our NNs can handle</p>
<p>So we pick some <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAJBAMAAAC7y9O3AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAoklEQVQIHWNggAL2AwwMTA0wHgOjsrNpAognysCgC6KFTIzDGBgqMg0YF4C4zAeYLgAp9rbJDFIMDALWDEwBIGEGGx0Qyci8gUEVSJ9lYFUA8RlYN4ApngsMQUDGDAb+AjDfBmw0A2cDw3egWR8Y5DhAwlCzGeoZGB80MDA/YDjLDBIGuuQuiLZj4LlgwMBkwJArAuSxA+3lbQAylBnYLjoAAKNFGt1X3xc1AAAAAElFTkSuQmCC" alt="n \times n" title="n \times n" /> square of pixels to check (where n is small) and have a neural network generate values for every <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAJBAMAAAC7y9O3AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAoklEQVQIHWNggAL2AwwMTA0wHgOjsrNpAognysCgC6KFTIzDGBgqMg0YF4C4zAeYLgAp9rbJDFIMDALWDEwBIGEGGx0Qyci8gUEVSJ9lYFUA8RlYN4ApngsMQUDGDAb+AjDfBmw0A2cDw3egWR8Y5DhAwlCzGeoZGB80MDA/YDjLDBIGuuQuiLZj4LlgwMBkwJArAuSxA+3lbQAylBnYLjoAAKNFGt1X3xc1AAAAAElFTkSuQmCC" alt="n \times n" title="n \times n" /> section of our full image. The result of this is effectively a smaller image, so we can do this again as described above</p>
<p>Each of these neural networks at each layer simply needs to learn a function that translates the <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAJBAMAAAC7y9O3AAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVHZmEKsyIkTvu92Zic2ZjesdAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAoklEQVQIHWNggAL2AwwMTA0wHgOjsrNpAognysCgC6KFTIzDGBgqMg0YF4C4zAeYLgAp9rbJDFIMDALWDEwBIGEGGx0Qyci8gUEVSJ9lYFUA8RlYN4ApngsMQUDGDAb+AjDfBmw0A2cDw3egWR8Y5DhAwlCzGeoZGB80MDA/YDjLDBIGuuQuiLZj4LlgwMBkwJArAuSxA+3lbQAylBnYLjoAAKNFGt1X3xc1AAAAAElFTkSuQmCC" alt="n \times n" title="n \times n" /> values that it sees to a useful summary of them for the resulting final layer neural network to give accurate values, and simply training these neural networks using the final layers output for the cost function produces exactly this</p>
<p>The reason that this works for images is that summaries of close sections tend to be useful as things that are close tend to be related, this method therefore does not work for any large dimensional problem</p>
<h2 id="transformers">Transformers</h2>
<p>Firstly, how can we encode text as numerical input?</p>
<h3 id="one-hot-encoding">One-Hot Encoding</h3>
<p>Simply list every possible word [cat,dog,mouse,rabbit,…]</p>
<p>Then, to represent some word, replace that word in the list with 1 and all others with 0</p>
<p>e.g. for mouse [0,0,1,0,…]</p>
<p>We do this instead of assigning the words to natural numbers as the natural numbers have undesired properties such as distance from one another and one value being greater than another, we don’t want the NN to incorrectly “pick up” on these</p>
<h3 id="nlp-natural-language-processing">NLP (Natural Language Processing)</h3>
<p>Natural language processing is another example of a high-dimensional input problem, each word (or any token) in our sentence is another dimension of our input, however this time physically close data isn’t necessarily related, for example “the maintenance worker that lived down the street from my grandma came around my house today”, in this sentence the phrases “maintenance worker” and “came around my house” are closely related even though they are on opposite sides of the sentence. If we made the distance assumption we may think that “grandma came around my house today”</p>
<p>So we need to consider a words “closeness” to every other word in the sentence regardless of how physically close they are</p>
<p>We can force this by using all the pairs of words in the sentence as opposed to just the words in the sentence, this completely removes the physical distance between any 2 words, we are going to want to store the words position in the sentence into its encoding as well as its one-hot however as it is still important information, that way we can have our cake and eat it too by keeping the position data but in a way that a CNN can handle</p>
<p>Taking a NN and using these pairs as 2 inputs allows us to “summarise” them into 1 output, we can then pair these up and do it all over again, creating layers similar to a CNN</p>
<p>However by the second layer, we are applying a NN to all the pairs of pairs of words, by the third it is all pairs of pairs of pairs, we are squaring the number of inputs from one layer to the next. We need to squash down the number of outputs at each layer to be the same as its number of inputs otherwise our model will be intractably large</p>
<p>We can do this by taking a linear combination of each n outputs, this combination would need to train to weigh important pairs high and unimportant pairs low so that the resulting sum is mostly the result of useful information and not noise</p>
<p>But for that to happen we need to have an importance score for each pair, this is the job of yet another NN, so we have one mapping pairs to their summaries, and another mapping them to an importance score</p>
<p>This is known as self-attention as our system is learning what pairs to pay attention to and the sentence is being compared to itSELF, that is both elements of the comparison pair are from the sentence</p>
<h3 id="generating-text">Generating text</h3>
<p>So we now have a model that can be influenced by relations between words in text regardless of physical distance of words, and that can work despite the seemingly very large dimensional input. We could use this model to, given a piece of text, provide likelihoods of each word in our dictionary to be the next word that would appear in that text</p>
<p>To train this we simply take sentences that are truncated, feed them in and check the models predicted probability of the actual word that came next, a lower probability of that word means a higher cost for that prediction (if we see a large enough variety of sentences, we will see e.g. I feel great/I feel wonderful, and even though one will get a higher probability than the other, they will both get high probabilities and so can both be selected)</p>
<p>With a model that can give us likelihoods for each word in our dictionary of appearing next, we can just take one of the most likely models and append it to the end of our text to have generated another word (One of the most likely as opposed to THE most likely so that we don’t always get the same response for the same input)</p>
<h4 id="auto-regression">Auto-Regression</h4>
<p>The great benefit of generating one token at a time is that the model gets to see the context it&#39;s generating into, if everything was generated at once the model would have to make a prediction based on all possible situations it could be in, for example all possible 4th words in an answer to some given question. Generating one word at a time means the 4th word would be generated with words 1 through 3 set in stone, this technique of utilising previous predictions for future predictions is called Auto-Regression</p>
<h3 id="answering-questions">Answering Questions</h3>
<p>So far our help bot has learnt language patterns from the internet, however, just because it looks correct, that doesn’t mean that it is useful, for example the request “How long is the average cat?” Could correctly be completed without grammatical error as “How long is the average cat? Is a question”, while that is correct that is not useful, so how do we make the model actually ask questions?</p>
<h4 id="instruction-tuning">Instruction Tuning</h4>
<p>We want our model to learn to answer question responses, instead of simply generating a plausible next word, so far it has just been learning general language on the internet, for Instruction Tuning, we use Question-Answer pairs as training data to reward our model for actually answering questions like we want it to (and for answering them correctly), we use a wide variety of ways of asking questions so that the model learns those patterns e.g. certain types of phrases and their common responses, it should be presented countless diverse tasks so that it can more easily generalise to new tasks via interpolation</p>
<h4 id="rlhf-reinforcement-learning-from-human-feedback">RLHF (Reinforcement Learning from Human Feedback)</h4>
<p>To make our model more useful, we also want it to learn what humans actually consider useful, therefore we will need human input to create a cost/reward function for our model</p>
<p>We can have users ask our model a question multiple times, and rank these responses on their usefulness</p>
<p>This now gives us labelled data that we can train a neural network on so that it can predict how some response would be scored by a human, this NN is usually the same transformer model as our ultimate model as we want them to have the same understanding of the language they are seeing, however it now is learning to output a score instead of a probability distribution</p>
<p>We can use this neural network now to score our model responses, essentially giving us another cost function that we can train our model on, this should improve our model’s perceived usefulness by our users</p>
<h3 id="ppo">PPO</h3>
<p>We need to ensure that our model doesn’t overfit to certain answers</p>
<h4 id="entropy-regularization">Entropy Regularization</h4>
<p>This sounds tricky, but really it is just adding some random value to our cost function of our responses</p>
<p>This prevents our model from locking into answers as quickly, it instead promotes more exploration rather than exploitation</p>
<p>The random addition is weighted so we can decide how much to explore or exploit</p>
<h4 id="clipping">Clipping</h4>
<p>When changing from one set of predictions (for our next word) to another, we check the percentage difference of our likelihood before the change and our likelihood after, If this percentage goes above a certain threshold, we simply reduce the percentage down to that threshold and instead shift our previous value by only that much.</p>
<p>This prevents massive changes to prevent overfitting to any single reward, it also prevents any sudden changes in behaviour of the model</p>
<h3 id="context-window">Context Window</h3>
<p>When you ask your chatbot something, instead of just considering your prompt, it also should take all your previous prompts and responses in your conversation for context, however we can only pass a limited amount of tokens into our model, the context window is just that size, the amount of history we include is <img style="vertical-align:middle" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAARBAMAAAAcfUzBAAAAMFBMVEX///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVGYiq4l2zRAy3ZnvRLscE2ieAAAACXBIWXMAAA7EAAAOxAGVKw4bAAADc0lEQVRIDa2UT2gcZRjGfzOzu5n9M5lJA9tGwX79Y1sj6CD0ZtkxoNLbbtsgouC01eBF3QqteBAnnkSUnR4Ei43JpWAOhZhS6aGHJV5MURgtObSNNZXSY7sQhMST7zdL0127DQ3xg2We73mf733eeb93FtZbuV8u2G+dw7zidajy9/yO3Ubgwju71Dp6O3oQFHwX3BBmwJlaCyyuoQ2AzyHrsZhw/ZGH+sMHIcGvineVfAxWshZ4ew1tANyBG1CAA4885IrN/SX4TRgOMO8z7edf3dvH263CrLz6euKzHUHBH2DfXOJqBylwuXv7WLt8E25L97rFxo5BjGOvU9638yALHx3C2BHy24XfL2n8PGamjoL8kyL6bLd0ZWygmepPz/CD/dN/co298CwLVw8llJ+OTdF/M3AQQ0jntZOK7afkuBzjsncmdWGEl/mRF61rczwF0s9X+CPvfafOa7w9OlOcMsRgvBQfOKuMJfpVJtB625tmnk+6vccbqqgGz2WDYmg1tf6E5EzJfl3/vX8i25vHfO9OqF3I1dlm1nFPm1PIEC0jz8M5JiOle/unr7LNcTFIKnjD5AKeIxNq/YTVsloc7/ZOFsm+9NXPjr8XVkWf5tRk4PpaeT1w5JAlA6RdcBWUFI24WJVanCaFW3sSdBmCmZyIci1RwBtwUWyZoxSn+mxgLunbcO7qlWr0scylrHR1Dnu1rZf5FnJGvku5a6OOjlpe26USQcWXiy0l8qLZgEZa4QmZyUAiA5irMbLm4Vdc31pmsq0vJFKJ0qGOdYuS1+chKrMl+lJsS/M0eQQmpM5AKLhG22USbPmdoiJVJX1qvCGpHGuFUDCNJZy/dW5jKpFubXXyLd53Ur0bFRIrLavDe0X+OhoRouoLRV/BrCdo8kPbOwqZhCccDJ9Qu1Dw2VWIzYC9FKuqEIeFmKFh83vTF4zrkU8/qWxVyS1fNGnaJ81UX/K/9MwOWw3zK3zMVgHT7BZbGY5iqFJyOqc/sT0walIeuSnJGcKqbYms2k44Rr4WGUd9TewfXBhCMBm5nGqatRblFF9v48bAt/tSvTP7zMi7OtSxcrNlnzEh9m9RiL6GUyMly4NUR0cTJIXtfXE+te04+D/AftUjSU+yh26TlOv3SNCT7KHbJCVD+PDqST4s2yTjfJrORneWnmQq+RcUL+yjtysFIwAAAABJRU5ErkJggg==" alt="contextWindow - \ promptSize" title="contextWindow - \ promptSize" /></p>
<p>This limited size means that, as your conversation progresses, your model will forget earlier parts of it</p>
<h3 id="prompt-engineering-and-system-instructions">Prompt Engineering and System Instructions</h3>
<p>We can modify the model’s behaviour through training, but we can also modify its behaviour by changing its context, simply by prepending some text to our prompt.</p>
<p>For example, adding “Without saying any swear words ” before a prompt would guide the behaviour of the response</p>
<p>This is Prompt Engineering, System Instructions are simply these engineered prompts that are prepended before the user gets to the prompt, not by the user themselves</p>
<h3 id="cot-chain-of-thought-training">CoT (Chain of Thought Training)</h3>
<p>Helps a model to reason</p>
<p>Simply change training data from</p>
<p>Question – Answer</p>
<p>To</p>
<p>Question – Reasoning Step by Step – Answer</p>
<p>This helps promote the model to break problems down into steps</p>
<p>This should go hand in hand with larger context windows, otherwise our model will forget what they were talking about and get lost</p>
<h1 id="asides">Asides</h1>
<h2 id="how-can-we-generalise-the-idea-of-line-segments-to-activation-functions-other-than-relu-why-do-they-still-work">How can we generalise the idea of line segments to activation functions other than ReLU? why do they still work?</h2>
<p>The input layer in the network just describes an n dimensional point, an input value</p>
<p>The first hidden layer takes a weighted sum and ReLUs it. These weights are the parameters of the ReLU function (as they are within the ReLU call I.e. ReLU(w*x)</p>
<p>This parameterised ReLU adds complexity by summing non-linearity, in the ReLU case it can sum line segments with different pivot points to make even more segments, in other activation functions an analogous complexity is combined.</p>
<p>The next layer does this same thing. Plugging a function into another just makes a new function, and in the same way we have added complexity, in the ReLU case by adding more line segments. A more complex function may now be modelled</p>
<p>Subsequent layers add complexity until we reach the output layer. This is the most complex function. In our line segment model, it is the function with the most line segments to make a function approximation, in general we are adding complexity to be able to create even more abstract functions with each layer</p>
<p>Intuitively you can think of this as each layer taking the large amount of input information it receives and giving a description of one particular part of it (which all your neighbouring neurons are doing about different parts). These abstractions can then be processed by the next layer to abstract even higher</p>
</body>
</html>
